{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "7hancdkh7sfqir67pct5",
   "authorId": "7637982313132",
   "authorName": "USER",
   "authorEmail": "",
   "sessionId": "18e30272-7171-4762-9c6b-0d6c5b2df4da",
   "lastEditTime": 1747940119095
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea401454-96a2-4798-b969-273e6351b1d3",
   "metadata": {
    "collapsed": false,
    "name": "cell1"
   },
   "source": "### 2.2. Collect Baseline Metrics\n\nModule 2 part 2 is to collect statistics of the base workload that are executed in the previous steps. \n\nLet's check the performance of the last workload run by using the `INFORMATION_SCHEMA.QUERY_HISTORY_BY_WAREHOUSE` function. We also extract the string text \"BASE WORKLOAD QUERY\" from the query string to identify the query number. And finally, we used the `ROW_NUMBER()` function to grab the latest run of each query. So it does not matter how many times you run for each query, the below script will always get the latest run of each of them."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": [
    "use warehouse WH_SUMMIT25_PERF_OPS; -- for operations & analysis \n",
    "select\n",
    "    query_id,\n",
    "    REGEXP_SUBSTR(query_text, 'BASE WORKLOAD QUERY - [0-9]{2}') as my_tag,\n",
    "    start_time,\n",
    "    execution_time\n",
    "from table(\n",
    "    INFORMATION_SCHEMA.QUERY_HISTORY_BY_WAREHOUSE(\n",
    "        WAREHOUSE_NAME =>'WH_SUMMIT25_PERF_BASE'\n",
    "    )\n",
    ")\n",
    "where \n",
    "    execution_time > 0\n",
    "    and query_text like '%BASE WORKLOAD QUERY%'\n",
    "qualify row_number() over (partition by my_tag order by start_time desc) = 1\n",
    "order by my_tag;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2b9eeb-9d0e-4dff-b65c-9aa67d7d7101",
   "metadata": {
    "collapsed": false,
    "name": "cell3"
   },
   "source": [
    "Let's also collect the stat details of each query into a table, so that we can analyze how well each query performed in terms of partitions scanned.\n",
    "\n",
    "This is achieved by creating a stored procedure to collect query stats using the `INFORMATION_SCHEMA.GET_QUERY_OPERATOR_STATS` function and store the stats we need into a table for analysis later on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24372cd-7a69-4144-a2c4-3b7be5685e63",
   "metadata": {
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE PROCEDURE insert_multiple_query_stats (\n",
    "    WH_NAME VARCHAR, \n",
    "    TARGET_TABLE_NAME VARCHAR,\n",
    "    NOTEBOOK_NAME VARCHAR\n",
    ")\n",
    "RETURNS TEXT\n",
    "LANGUAGE JAVASCRIPT\n",
    "EXECUTE AS CALLER\n",
    "AS \n",
    "$$    \n",
    "    // Get all query IDs using CTE    \n",
    "    var query_ids_sql = `\n",
    "        SELECT \n",
    "            DISTINCT \n",
    "            query_id, \n",
    "            REGEXP_SUBSTR(query_text, '[A-Z]{1,} WORKLOAD QUERY - [0-9]{2}') as my_tag, \n",
    "            start_time, \n",
    "            warehouse_name   \n",
    "        FROM TABLE(INFORMATION_SCHEMA.QUERY_HISTORY_BY_WAREHOUSE(WAREHOUSE_NAME =>'${WH_NAME}', RESULT_LIMIT =>10000))\n",
    "        WHERE \n",
    "            parse_json(query_tag):\"StreamlitName\" = 'SQL_PERF_OPTIMIZATION.PUBLIC.${NOTEBOOK_NAME}' \n",
    "            and query_text like '%WORKLOAD QUERY%'\n",
    "            and query_type = 'SELECT'\n",
    "    `;\n",
    "\n",
    "    var query_ids_stmt = snowflake.createStatement({        \n",
    "    \tsqlText: query_ids_sql \n",
    "    });\n",
    "\n",
    "    var query_ids_result = query_ids_stmt.execute();    \n",
    "    var processed = 0;\n",
    "    var skipped = 0;\n",
    "    var inserted = 0;\n",
    "\n",
    "    snowflake.createStatement({\n",
    "        sqlText: `create table if not exists SQL_PERF_OPTIMIZATION.PUBLIC.${TARGET_TABLE_NAME} (\n",
    "            query_id string, \n",
    "            start_time timestamp_ntz(0),\n",
    "            operator_type string, \n",
    "            execution_time_breakdown string, \n",
    "            operator_attributes variant, \n",
    "            operator_id string,\n",
    "            operator_statistics variant, \n",
    "            parent_operators array, \n",
    "            step_id string, \n",
    "            query_tag string\n",
    "        );`\n",
    "    }).execute();\n",
    "\n",
    "    while (query_ids_result.next()) {\n",
    "        var current_query_id = query_ids_result.getColumnValueAsString(1);    \n",
    "        var current_query_tag = query_ids_result.getColumnValueAsString(2);\n",
    "        var current_start_time = query_ids_result.getColumnValueAsString(3);\n",
    "        var exists_check_query = \n",
    "            \"SELECT COUNT(1) FROM SQL_PERF_OPTIMIZATION.PUBLIC.\" + TARGET_TABLE_NAME + \" WHERE query_id = ?\";\n",
    "        var exists_stmt = snowflake.createStatement({            \n",
    "            sqlText: exists_check_query,            \n",
    "            binds: [ current_query_id ]\n",
    "        });\n",
    "        var exists_result = exists_stmt.execute();        \n",
    "        exists_result.next();                \n",
    "        if (exists_result.getColumnValue(1) === 0) {\n",
    "            var query_id\n",
    "            var insert_query = `\n",
    "                INSERT INTO SQL_PERF_OPTIMIZATION.PUBLIC.${TARGET_TABLE_NAME}\n",
    "                (\n",
    "                    operator_type, execution_time_breakdown, operator_attributes, operator_id,\n",
    "                    operator_statistics, parent_operators, query_id, start_time, step_id, query_tag\n",
    "                )\n",
    "                SELECT \n",
    "                    operator_type, execution_time_breakdown, \n",
    "                    operator_attributes, operator_id, \n",
    "                    operator_statistics, parent_operators,\n",
    "                    query_id, '${current_start_time}', \n",
    "                    step_id, '${current_query_tag}'\n",
    "                FROM TABLE(INFORMATION_SCHEMA.GET_QUERY_OPERATOR_STATS(?))\n",
    "            `;\n",
    "\n",
    "            var insert_stmt = snowflake.createStatement({                \n",
    "                sqlText: insert_query,                \n",
    "                binds: [current_query_id] });                        \n",
    "            insert_stmt.execute();            \n",
    "            inserted++;        \n",
    "        } \n",
    "        else {            \n",
    "            skipped++;\n",
    "        }        \n",
    "        processed++;    \n",
    "    }       \n",
    "\n",
    "    return \"Processing complete. Total processed:  \"+processed+\", Inserted:  \"+ inserted +\", Skipped: \"+skipped ;\n",
    "$$\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c66eb9-f58e-4ff8-ae3b-d777ad187229",
   "metadata": {
    "collapsed": false,
    "name": "cell5"
   },
   "source": [
    "Once the SP is ready, let's generate and collect the stats for our last run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a234bf6-8ac3-4f53-af3e-f56c4ed56dc3",
   "metadata": {
    "language": "sql",
    "name": "cell6"
   },
   "outputs": [],
   "source": [
    "CALL insert_multiple_query_stats(\n",
    "    'WH_SUMMIT25_PERF_BASE', \n",
    "    'BASE_QUERY_STATS', \n",
    "    'MODULE2_PART1_BASE_WORKLOAD'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a3e2cc-810a-47bf-8211-e8040c096159",
   "metadata": {
    "collapsed": false,
    "name": "cell7"
   },
   "source": [
    "The below query will scan through the data that we collected in the `BASE_QUERY_STATS` table and find out which tables we scanned more than 80% of micro-partition files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc1de5c-89e6-4858-846a-5ef47e2fe1c8",
   "metadata": {
    "language": "sql",
    "name": "cell8"
   },
   "outputs": [],
   "source": [
    "with latest_query_each_tag as (\n",
    "    select query_id\n",
    "    from base_query_stats\n",
    "    qualify row_number() over (partition by query_tag order by start_time desc) = 1\n",
    ")\n",
    "select \n",
    "    distinct\n",
    "    s.query_id,\n",
    "    query_tag,\n",
    "    operator_attributes:table_name::string as table_name,\n",
    "    operator_statistics:pruning:partitions_scanned as mp_scanned,\n",
    "    operator_statistics:pruning:partitions_total as mp_total,\n",
    "    round(mp_scanned/mp_total, 4) * 100 as scan_rate\n",
    "from base_query_stats s\n",
    "join latest_query_each_tag q on (\n",
    "    s.query_id = q.query_id\n",
    ")\n",
    "where \n",
    "    mp_total is not null\n",
    "    and scan_rate > 80\n",
    "order by query_tag\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3603358a-3ec5-4b98-b6f1-abe80a021768",
   "metadata": {
    "collapsed": false,
    "name": "cell9"
   },
   "source": [
    "From the result above, we can see that most of the scans, in fact all of them, against the tables involved in the workload were FULL TABLE SCANs. This is not ideal and means that our tables were not properly clustered based on the filters we used in those queries.\n",
    "\n",
    "Please go back to the quickstart guide and follow the section 2.3 to add some of the monitoring queries into Snowflake Dashboard for query performance analysis."
   ]
  }
 ]
}